{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural - Trabalho Pr√°tico 1\n",
    "### Tha√≠s Ferreira da Silva - 2021092571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "O objetivo desse trabalho √© entender e aplicar conceitos de word embeddings, analisando a qualidade dos modelos treinados em fun√ß√£o dos par√¢metros escolhidos para o seu treinamento.\n",
    "\n",
    "### Atividades Principais\n",
    "Para esse trabalho, utilizei o gensim no treinamento do modelo atraves da varia√ß√£o dos par√¢metros do Word2Vec.\n",
    "\n",
    "Dividi esse trabalho em 4 partes principais:\n",
    "\n",
    "##### Fun√ß√µes auxiliares: \n",
    "Parte contendo todas os m√©todos implementados para as partes seguintes\n",
    "\n",
    "##### Treinamento: \n",
    "Aqui defini os parametros de treinamento e realizei de fato o treinamento dos 54 modelos resultado da varia√ß√£o dos par√¢metos: \n",
    "- tamanho do vetor de palavras\n",
    "- tamanho da janela de contexto\n",
    "- n√∫mero de intera√ß√µes\n",
    "- utiliza CBOW ou skip-gram. \n",
    "\n",
    "Realizei o treinamento 2 vezes com par√¢metros diferentes e armazenei em 2 pastas separadas para futuras analises de desempenho.\n",
    "\n",
    "##### Avalia√ß√£o: \n",
    "Nessa parte utilizei a dist√¢ncia m√©dia do cosseno (metrica exigida pelo trabalho) e a acur√°cia do modelo para escolher o melhor modelo treinado de acordo com as predi√ß√µes sobre analogias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports do gensim - para word2vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "# Imports para graficos\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports extras\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "## Fun√ß√µes Auxiliares\n",
    "\n",
    "Optei por desenvolver 4 principais fun√ß√µes com os seguintes objetivos:\n",
    "\n",
    "- Gerar as combina√ßoes de hiperparametros: S√£o 54 varia√ß√µes, onde 3 par√¢metros possuem 3 varia√ß√µes, e o for fim temos a varia√ß√£o do skip-gram e CBOW\n",
    "\n",
    "- Treinar e salvar os modelos: Os modelos s√£o treinados com Word2Vac e para facilitar as analises ap√≥s o treinamento de cada modelo, eles s√£o salvos em 2 pastas diferentes\n",
    "\n",
    "- Pegar as analogias: Aqui o arquivo de texto com as analogias s√£o lidos de 4 em 4 palavras e armazenados em um vetor que posteriormente ser√° utilizado para a previs√£o e avalia√ß√£o dos modelos\n",
    "\n",
    "- Avaliar o model: Na avalia√ß√£o dos modelos, √© necess√°rio passar por todas as analogias, e realizar a opera√ß√£o vetorial e prever o 4¬∞ elemento do vetor. Dessa forma, ao final de todo o processamento temos os calculos das m√©tricas de avalia√ß√£o, sendo elas a dist√¢ncia de cosseno e a acur√°cia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hyperparameter_combinations(param_grid):\n",
    "    from itertools import product\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    return combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(corpus, params, output_dir):\n",
    "    model_name = f\"word2vec_vs{params['vector_size']}_win{params['window']}_sg{params['sg']}_ep{params['epochs']}\"\n",
    "    print(f\"Treinando modelo: {model_name}\")\n",
    "    \n",
    "    model = Word2Vec(\n",
    "        sentences=corpus,\n",
    "        vector_size=params['vector_size'],\n",
    "        window=params['window'],\n",
    "        sg=params['sg'],\n",
    "        epochs=params['epochs'],\n",
    "        workers=8 #cores do processador\n",
    "    )\n",
    "    \n",
    "    model_path = os.path.join(output_dir, f\"{model_name}.model\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Modelo salvo: {model_path}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogies(model, questions_words_path):\n",
    "    final_analogies = []\n",
    "    with open(questions_words_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(':'):\n",
    "                continue\n",
    "\n",
    "            words = [word.lower().strip() for word in line.split()]\n",
    "\n",
    "            final_words = [word for word in words if word in model.wv]\n",
    "            if len(final_words) != 4:\n",
    "                continue\n",
    "            \n",
    "            final_analogies.append(final_words)\n",
    "    \n",
    "    random.shuffle(final_analogies)\n",
    "    return final_analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model, analogies):\n",
    "    avg_distance = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for analogy in analogies:\n",
    "        if len(analogy) == 4:\n",
    "            count += 1\n",
    "\n",
    "            try:\n",
    "                result_vector = model.wv[analogy[1]] - model.wv[analogy[0]] + model.wv[analogy[2]]\n",
    "\n",
    "                predicted = model.wv.similar_by_vector(result_vector, topn=20, restrict_vocab=None)\n",
    "                predicted_word = next((word for word, _ in predicted if word not in analogy[:3]), None)\n",
    "\n",
    "                if predicted_word == analogy[3]:\n",
    "                    correct += 1\n",
    "\n",
    "                if predicted_word in model.wv:\n",
    "                    cosine_distance = cosine(model.wv[analogy[3]], model.wv[predicted_word])\n",
    "                    avg_distance += cosine_distance\n",
    "\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "\n",
    "    accuracy = correct / count if count > 0 else 0\n",
    "    avg_distance = avg_distance / count if count > 0 else 0\n",
    "\n",
    "    return accuracy, avg_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_evaluate_models(model, analogies, model_name):\n",
    "    avg_distance = 0  # M√©dia da dist√¢ncia cosseno\n",
    "    total_loss = 0  # Loss total (pode ser dist√¢ncia cosseno ou MSE)\n",
    "    correct = 0  # Contagem de analogias corretas\n",
    "    count = 0  # Total de analogias processadas\n",
    "    results = []  # Lista para armazenar os detalhes de cada analogia\n",
    "    start_time = time.time()\n",
    "\n",
    "    for analogy in analogies:\n",
    "        if len(analogy) == 4:\n",
    "            count += 1\n",
    "\n",
    "            try:\n",
    "                # Calcula o vetor resultante da analogia\n",
    "                result_vector = model.wv[analogy[1]] - model.wv[analogy[0]] + model.wv[analogy[2]]\n",
    "\n",
    "                # Prediz a palavra com base no vetor calculado\n",
    "                predicted = model.wv.similar_by_vector(result_vector, topn=20, restrict_vocab=None)\n",
    "                predicted_word = next((word for word, _ in predicted if word not in analogy[:3]), None)\n",
    "\n",
    "                # Verifica se a previs√£o est√° correta\n",
    "                is_correct = predicted_word == analogy[3]\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "\n",
    "                # Calcula a dist√¢ncia cosseno entre a palavra esperada e a palavra predita\n",
    "                if predicted_word in model.wv:\n",
    "                    cosine_distance = cosine(model.wv[analogy[3]], model.wv[predicted_word])\n",
    "                    avg_distance += cosine_distance\n",
    "\n",
    "                # Calcula a loss como a dist√¢ncia cosseno entre o vetor resultante e o vetor esperado\n",
    "                if analogy[3] in model.wv:\n",
    "                    cosine_loss = cosine(model.wv[analogy[3]], result_vector)  # Loss usando dist√¢ncia cosseno\n",
    "                    total_loss += cosine_loss\n",
    "\n",
    "                # Armazena os detalhes da analogia, incluindo todas as palavras preditas\n",
    "                results.append({\n",
    "                    'sg': 0,\n",
    "                    'vector_size': 0,\n",
    "                    'window': 0,\n",
    "                    'epochs': 0,\n",
    "                    'analogy': f'{analogy[0]}:{analogy[1]}::{analogy[2]}:{analogy[3]}',\n",
    "                    'word_a': analogy[0],\n",
    "                    'word_b': analogy[1],\n",
    "                    'word_c': analogy[2],\n",
    "                    'expected_word': analogy[3],\n",
    "                    'predicted_word': predicted_word,\n",
    "                    'all_predicted_words': [word for word, _ in predicted],  # Lista de todas as palavras preditas\n",
    "                    'correct': is_correct,\n",
    "                    'accuracy': 0,\n",
    "                    'avg_distance': 0,\n",
    "                    'cosine_loss': cosine_loss  # Loss para esta analogia\n",
    "                })\n",
    "\n",
    "            except KeyError as e:\n",
    "                print(f\"Palavra n√£o encontrada no vocabul√°rio: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Calcula as m√©tricas globais\n",
    "    accuracy = correct / count if count > 0 else 0\n",
    "    avg_distance = avg_distance / count if count > 0 else 0\n",
    "    avg_loss = total_loss / count if count > 0 else 0  # Calcula a m√©dia da loss para todas as analogias\n",
    "\n",
    "    # Converte a lista de resultados em um DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    return accuracy, avg_distance, avg_loss, df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CBOWxSkipGram(df_final):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    sns.countplot(data=df_final, x='sg', hue='correct', ax=axes[0])\n",
    "    axes[0].set_title('N√∫mero de acertos e erros por tipo de sg')\n",
    "\n",
    "    sns.scatterplot(data=df_final, x='accuracy', y='avg_distance', hue='sg', ax=axes[1])\n",
    "    axes[1].set_title('Acur√°cia x Dist√¢ncia M√©dia')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_embeddings_impact(df_final):\n",
    "    #plotar e mostrar o impacto do tamanho do embedding no valor da acur√°cia e da dist√¢ncia m√©dia\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    sns.scatterplot(data=df_final, x='vector_size', y='accuracy', hue='sg', ax=axes[0])\n",
    "    axes[0].set_title('Tamanho do Embedding x Acur√°cia')\n",
    "\n",
    "    sns.scatterplot(data=df_final, x='vector_size', y='avg_distance', hue='sg', ax=axes[1])\n",
    "    axes[1].set_title('Tamanho do Embedding x Dist√¢ncia M√©dia')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #Gr√°fico de Linhas (Evolu√ß√£o da Acur√°cia e Dist√¢ncia M√©dia)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    sns.lineplot(data=df_final, x='epochs', y='accuracy', hue='sg', ax=axes[0])\n",
    "    axes[0].set_title('Evolu√ß√£o da Acur√°cia por √âpoca')\n",
    "\n",
    "    sns.lineplot(data=df_final, x='epochs', y='avg_distance', hue='sg', ax=axes[1])\n",
    "    axes[1].set_title('Evolu√ß√£o da Dist√¢ncia M√©dia por √âpoca')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Boxplot (Distribui√ß√£o das Dist√¢ncias)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    sns.boxplot(data=df_final, x='sg', y='avg_distance', ax=axes[0])\n",
    "    axes[0].set_title('Distribui√ß√£o das Dist√¢ncias por SG')\n",
    "\n",
    "    sns.boxplot(data=df_final, x='sg', y='accuracy', ax=axes[1])\n",
    "    axes[1].set_title('Distribui√ß√£o das Acur√°cias por SG')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Gr√°fico de Pareto (Impacto do Embedding)\n",
    "    df_final['vector_size'] = df_final['vector_size'].astype(str)\n",
    "    df_final['vector_size'] = df_final['vector_size'] + 'D'\n",
    "\n",
    "    df_pareto = df_final.groupby('vector_size').agg({'accuracy': 'mean', 'avg_distance': 'mean'}).reset_index()\n",
    "\n",
    "    df_pareto['accuracy'] = df_pareto['accuracy'] * 100\n",
    "    df_pareto['avg_distance'] = df_pareto['avg_distance'] * 100\n",
    "\n",
    "    df_pareto = df_pareto.sort_values(by='avg_distance', ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "    sns.barplot(data=df_pareto, x='vector_size', y='avg_distance', color='red', ax=ax)\n",
    "    ax.set_title('Impacto do Embedding na Dist√¢ncia M√©dia')\n",
    "    ax.set_ylabel('Dist√¢ncia M√©dia (%)')\n",
    "    ax.set_xlabel('Tamanho do Embedding')\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    sns.lineplot(data=df_pareto, x='vector_size', y='accuracy', color='blue', ax=ax2)\n",
    "    ax2.set_ylabel('Acur√°cia (%)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(df_results):\n",
    "    # Cria uma figura com 3 subplots (1 linha e 3 colunas)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # 1 linha, 3 colunas\n",
    "\n",
    "    # === 1. Histograma ===\n",
    "    axes[0].hist(df_results['cosine_loss'], bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[0].set_xlabel('Cosine Loss')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of Loss for Analogies')\n",
    "\n",
    "    # === 2. Gr√°fico de Linha ===\n",
    "    axes[1].plot(df_results['cosine_loss'], color='red')\n",
    "    axes[1].set_xlabel('Index of Analogy')\n",
    "    axes[1].set_ylabel('Cosine Loss')\n",
    "    axes[1].set_title('Loss for Each Analogy')\n",
    "\n",
    "    # === 3. Boxplot ===\n",
    "    sns.boxplot(x=df_results['cosine_loss'], ax=axes[2], color='lightgreen')\n",
    "    axes[2].set_title('Boxplot of Loss for Analogies')\n",
    "\n",
    "    # Ajusta o layout para que n√£o haja sobreposi√ß√£o de textos\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "## Inicializa√ß√£o\n",
    "Aqui definimos lemos o arquivo do courpus, e definimos as pastas onde os modelos ser√£o armazenados futuramente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text8_path = './text8'\n",
    "corpus = Text8Corpus(text8_path)\n",
    "\n",
    "sentence = next(iter(corpus))\n",
    "print(sentence[:15])\n",
    "\n",
    "output_dir = './word2vec_models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir1 = './word2vec_models_v1'\n",
    "os.makedirs(output_dir1, exist_ok=True)\n",
    "\n",
    "output_dir2 = './word2vec_models_v2'\n",
    "os.makedirs(output_dir2, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "# Treinamento dos modelos Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperpar√¢metros para o GridSearch\n",
    "param_grid = {\n",
    "    'vector_size': [50, 100, 200],      # Tamanho do vetor de palavras\n",
    "    'window': [3, 5, 7],                # Tamanho da janela de contexto\n",
    "    'sg': [0, 1],                      # CBOW (0) ou Skip-gram (1)\n",
    "    'epochs': [5, 10, 15],             # N√∫mero de itera√ß√µes de treinamento\n",
    "}\n",
    "\n",
    "combinations = generate_hyperparameter_combinations(param_grid)\n",
    "\n",
    "for i, params in enumerate(combinations):\n",
    "    print(f\"\\nTreinando combina√ß√£o {i+1}/{len(combinations)}: {params}\")\n",
    "    model = train_and_save_model(corpus, params, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperpar√¢metros para o GridSearch\n",
    "param_grid1 = {\n",
    "    'vector_size': [150, 250, 300],      # Tamanho do vetor de palavras\n",
    "    'window': [3, 5, 7],                # Tamanho da janela de contexto\n",
    "    'sg': [0, 1],                      # CBOW (0) ou Skip-gram (1)\n",
    "    'epochs': [5, 10, 15],             # N√∫mero de itera√ß√µes de treinamento\n",
    "}\n",
    "\n",
    "combinations1 = generate_hyperparameter_combinations(param_grid1)\n",
    "\n",
    "for j, params in enumerate(combinations1):\n",
    "    print(f\"\\nTreinando combina√ß√£o {j+1}/{len(combinations1)}: {params}\")\n",
    "    model1 = train_and_save_model(corpus, params, output_dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperpar√¢metros para o GridSearch\n",
    "param_grid2 = {\n",
    "    'vector_size': [50, 100, 200],      # Tamanho do vetor de palavras\n",
    "    'window': [3, 5, 7],                # Tamanho da janela de contexto\n",
    "    'sg': [0, 1],                      # CBOW (0) ou Skip-gram (1)\n",
    "    'epochs': [10, 20, 30],             # N√∫mero de itera√ß√µes de treinamento\n",
    "}\n",
    "\n",
    "combinations2 = generate_hyperparameter_combinations(param_grid2)\n",
    "\n",
    "for j, params in enumerate(combinations2):\n",
    "    print(f\"\\nTreinando combina√ß√£o {j+1}/{len(combinations2)}: {params}\")\n",
    "    model2 = train_and_save_model(corpus, params, output_dir2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "## Avalia√ß√£o dos modelos\n",
    "### Modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_words_path = './questions-words.txt'\n",
    "\n",
    "models = os.listdir(output_dir)\n",
    "models = [model for model in models if model.endswith('.model')]\n",
    "\n",
    "model_metrics = []\n",
    "all_results = []\n",
    "\n",
    "num_modelo = 0\n",
    "\n",
    "for model_name in models:\n",
    "    num_modelo += 1\n",
    "    model = Word2Vec.load(os.path.join(output_dir, model_name))\n",
    "    analogies = get_analogies(model, questions_words_path)\n",
    "    accuracy, avg_distance, avg_loss, df_results = new_evaluate_models(model, analogies, model_name)\n",
    "    \n",
    "    print(f\"Modelo: [{num_modelo}]\", end='\\r')\n",
    "    \n",
    "    model_metrics.append({\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_distance': avg_distance,\n",
    "        'vector_size': model.vector_size,\n",
    "        'window': model.window,\n",
    "        'sg': model.sg,\n",
    "        'epochs': model.epochs\n",
    "    })\n",
    "\n",
    "    df_results['sg'] = 'CBOW' if model.sg == 0 else 'SkipGram'\n",
    "    df_results['vector_size'] = model.vector_size\n",
    "    df_results['window'] = model.window\n",
    "    df_results['epochs'] = model.epochs\n",
    "\n",
    "    #trocar todos os valores 0 de acuracia e avg_distance no df_results para esse modelo pelo seu valor real\n",
    "    df_results['accuracy'] = accuracy\n",
    "    df_results['avg_distance'] = avg_distance\n",
    "    df_results['avg_loss'] = avg_loss\n",
    "        \n",
    "    # Armazenar o DataFrame na lista de todos os resultados\n",
    "    all_results.append(df_results)\n",
    "\n",
    "# Concatenar todos os DataFrames de resultados em um √∫nico DataFrame\n",
    "final_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Salvar o arquivo CSV de resultados na **raiz do projeto**\n",
    "final_df.to_csv('./results/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_words_path = './questions-words.txt'\n",
    "\n",
    "models1 = os.listdir(output_dir1)\n",
    "models1 = [model1 for model1 in models1 if model1.endswith('.model')]\n",
    "\n",
    "model_metrics1 = []\n",
    "all_results1 = []\n",
    "\n",
    "num_modelo1 = 0\n",
    "\n",
    "for model_name1 in models1:\n",
    "    num_modelo1 +=1\n",
    "    model1 = Word2Vec.load(os.path.join(output_dir1, model_name1))\n",
    "    analogies1 = get_analogies(model1, questions_words_path)\n",
    "    accuracy1, avg_distance1, avg_loss1, df_results1 = new_evaluate_models(model1, analogies1, model_name1)\n",
    "    \n",
    "    print(f\"Modelo: [{num_modelo1}]\", end='\\r')\n",
    "    \n",
    "    model_metrics1.append({\n",
    "        'model_name': model_name1,\n",
    "        'accuracy': accuracy1,\n",
    "        'avg_distance': avg_distance1,\n",
    "        'vector_size': model1.vector_size,\n",
    "        'window': model1.window,\n",
    "        'sg': model1.sg,\n",
    "        'epochs': model1.epochs\n",
    "    })\n",
    "\n",
    "    df_results1['sg'] = 'CBOW' if model1.sg == 0 else 'SkipGram'\n",
    "    df_results1['vector_size'] = model1.vector_size\n",
    "    df_results1['window'] = model1.window\n",
    "    df_results1['epochs'] = model1.epochs\n",
    "\n",
    "    #trocar todos os valores 0 de acuracia e avg_distance no df_results para esse modelo pelo seu valor real\n",
    "    df_results1['accuracy'] = accuracy1\n",
    "    df_results1['avg_distance'] = avg_distance1\n",
    "    df_results1['avg_loss'] = avg_loss1\n",
    "        \n",
    "    # Armazenar o DataFrame na lista de todos os resultados\n",
    "    all_results1.append(df_results1)\n",
    "\n",
    "# Concatenar todos os DataFrames de resultados em um √∫nico DataFrame\n",
    "final_df1 = pd.concat(all_results1, ignore_index=True)\n",
    "\n",
    "# Salvar o arquivo CSV de resultados na **raiz do projeto**\n",
    "final_df1.to_csv('./results/results1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./results/results1.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_words_path = './questions-words.txt'\n",
    "\n",
    "models2 = os.listdir(output_dir2)\n",
    "models2 = [model2 for model2 in models2 if model2.endswith('.model')]\n",
    "\n",
    "model_metrics2 = []\n",
    "all_results2 = []\n",
    "\n",
    "num_modelo2 = 0\n",
    "\n",
    "for model_name2 in models2:\n",
    "    num_modelo2 +=1\n",
    "    model2 = Word2Vec.load(os.path.join(output_dir2, model_name2))\n",
    "    analogies2 = get_analogies(model2, questions_words_path)\n",
    "    accuracy2, avg_distance2, avg_loss2, df_results2 = new_evaluate_models(model2, analogies2, model_name2)\n",
    "    \n",
    "    print(f\"Modelo: [{num_modelo2}]\", end='\\r')\n",
    "    \n",
    "    model_metrics2.append({\n",
    "        'model_name': model_name2,\n",
    "        'accuracy': accuracy2,\n",
    "        'avg_distance': avg_distance2,\n",
    "        'vector_size': model2.vector_size,\n",
    "        'window': model2.window,\n",
    "        'sg': model2.sg,\n",
    "        'epochs': model2.epochs\n",
    "    })\n",
    "\n",
    "    df_results2['sg'] = 'CBOW' if model2.sg == 0 else 'SkipGram'\n",
    "    df_results2['vector_size'] = model2.vector_size\n",
    "    df_results2['window'] = model2.window\n",
    "    df_results2['epochs'] = model2.epochs\n",
    "\n",
    "    #trocar todos os valores 0 de acuracia e avg_distance no df_results para esse modelo pelo seu valor real\n",
    "    df_results2['accuracy'] = accuracy2\n",
    "    df_results2['avg_distance'] = avg_distance2\n",
    "    df_results2['avg_loss'] = avg_loss2\n",
    "        \n",
    "    # Armazenar o DataFrame na lista de todos os resultados\n",
    "    all_results2.append(df_results2)\n",
    "\n",
    "# Concatenar todos os DataFrames de resultados em um √∫nico DataFrame\n",
    "final_df2 = pd.concat(all_results2, ignore_index=True)\n",
    "\n",
    "# Salvar o arquivo CSV de resultados na **raiz do projeto**\n",
    "final_df2.to_csv('./results/results2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./results/results2.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#juntar os 3 df\n",
    "df_final = pd.concat([df, df1, df2], ignore_index=True)\n",
    "df_final.to_csv('./results/results_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mostrar o top 10 dos melhores modelos de acordo com a distancia media\n",
    "df_final.sort_values(by='avg_distance').head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mostrar o top 10 dos melhores modelos de acordo com a acuracia\n",
    "df_final.sort_values(by='accuracy', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## Analise dos resultados\n",
    "### Impacto da arquitetura: CBOW (Continuous Bag of Words) x Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CBOWxSkipGram(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impacto do tamanho do embedding / vector_size - acur√°cia x dist√¢ncia de cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings_impact(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impacto da quantidade de √©pocas - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "\n",
    "1. Histograma de Cosine Loss\n",
    "\n",
    "    Como interpretar?\n",
    "    O histograma mostra a distribui√ß√£o de frequ√™ncia dos valores de loss para todas as analogias testadas.\n",
    "\n",
    "O que observar?\n",
    "\n",
    "    Distribui√ß√£o concentrada perto de zero:\n",
    "        Se a maioria dos valores estiver pr√≥xima de zero, isso indica que as analogias est√£o sendo bem resolvidas.\n",
    "        Um pico forte pr√≥ximo de zero significa que muitas analogias t√™m vetores preditos pr√≥ximos aos vetores esperados.\n",
    "\n",
    "    Distribui√ß√£o larga ou com muitos valores acima de 1:\n",
    "        Isso sugere que o modelo tem dificuldade em resolver muitas analogias.\n",
    "        Muitas perdas altas (>1) podem indicar que o modelo n√£o aprendeu bem as rela√ß√µes sem√¢nticas entre as palavras.\n",
    "\n",
    "    Outliers (valores fora do padr√£o):\n",
    "        Outliers (valores muito acima de 1.5) indicam casos de analogias em que o vetor predito est√° muito distante do vetor correto.\n",
    "        Pode ser interessante identificar quais analogias est√£o associadas a essas losses altas para entender onde o modelo falha.\n",
    "\n",
    "    Exemplo de interpreta√ß√£o:\n",
    "    Se o histograma mostrar uma distribui√ß√£o que parece uma curva normal (em forma de sino) centrada em 0.2, isso significa que, em m√©dia, o erro entre os vetores esperados e preditos √© pequeno. Se houver uma cauda longa (valores maiores que 1), isso indica algumas falhas significativas.\n",
    "\n",
    "üìà 2. Gr√°fico de Linha de Cosine Loss\n",
    "\n",
    "    Como interpretar?\n",
    "    O gr√°fico de linha mostra a evolu√ß√£o da loss para cada analogia, ou seja, cada ponto no eixo x representa uma analogia processada, e o eixo y mostra o valor da loss.\n",
    "\n",
    "O que observar?\n",
    "\n",
    "    Tend√™ncias ou picos:\n",
    "        Se houver picos muito altos em algumas partes, o modelo pode estar falhando em um subconjunto espec√≠fico de analogias.\n",
    "        Se o gr√°fico estiver relativamente est√°vel e baixo, isso √© um bom sinal, pois mostra que o modelo est√° consistentemente resolvendo as analogias.\n",
    "\n",
    "    Padr√µes C√≠clicos:\n",
    "        Se houver padr√µes repetidos (por exemplo, o erro aumenta e diminui periodicamente), isso pode indicar que o modelo est√° lidando de forma inconsistente com algumas classes de palavras.\n",
    "        Isso pode acontecer se as analogias forem agrupadas por categorias (por exemplo, pa√≠ses e capitais ou palavras de g√™nero).\n",
    "\n",
    "    M√©dia geral:\n",
    "        Se o gr√°fico est√° sempre abaixo de 0.5, significa que o erro m√©dio de predi√ß√£o √© pequeno, o que √© positivo.\n",
    "        Se a maior parte das losses estiver acima de 1, o modelo n√£o est√° conseguindo capturar as rela√ß√µes esperadas entre as palavras.\n",
    "\n",
    "    Exemplo de interpreta√ß√£o:\n",
    "    Imagine que o gr√°fico mostre pequenos picos, mas que a maioria das losses est√° abaixo de 0.5. Isso indica que o modelo est√° consistentemente bem.\n",
    "    Se o gr√°fico tiver v√°rias spikes (picos) altos, pode ser √∫til verificar as palavras associadas a essas perdas e entender por que o modelo est√° falhando nessas analogias espec√≠ficas.\n",
    "\n",
    "üì¶ 3. Boxplot de Cosine Loss\n",
    "\n",
    "    Como interpretar?\n",
    "    O boxplot mostra a distribui√ß√£o e os outliers da loss de forma compacta. Ele exibe o m√≠nimo, o primeiro quartil (Q1), a mediana, o terceiro quartil (Q3) e o m√°ximo, al√©m dos outliers (pontos fora do padr√£o).\n",
    "\n",
    "O que observar?\n",
    "\n",
    "    Mediana (linha no meio do boxplot):\n",
    "        A posi√ß√£o da mediana indica a loss t√≠pica para o conjunto de analogias.\n",
    "        Se a mediana for pr√≥xima de zero, √© um bom sinal de que a maioria das analogias foi resolvida corretamente.\n",
    "\n",
    "    Comprimento do \"box\" (Q1 a Q3):\n",
    "        Se o box for estreito (entre Q1 e Q3), isso indica que a maioria das analogias tem perdas muito semelhantes.\n",
    "        Se o box for largo, as losses est√£o muito espalhadas, o que pode indicar inconsist√™ncia do modelo.\n",
    "\n",
    "    Outliers (pontos fora do boxplot):\n",
    "        Outliers s√£o analogias que tiveram losses muito acima do normal.\n",
    "        Esses outliers podem ser causados por palavras fora do vocabul√°rio ou rela√ß√µes sem√¢nticas que o modelo n√£o aprendeu bem.\n",
    "        Identificar quais analogias est√£o associadas a esses outliers pode ajudar a entender quais categorias de analogias o modelo tem dificuldade de resolver."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
